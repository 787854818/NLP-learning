{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [   '[document]',   'noscript', 'header',   'html', 'meta', 'head','input', 'script',   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "def epub2thtml(epub_path):\n",
    "    book = epub.read_epub(epub_path)\n",
    "    chapters = []\n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            chapters.append(item.get_content())\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chap2text(chap):\n",
    "    output = ''\n",
    "    soup = BeautifulSoup(chap, 'html.parser')\n",
    "    text = soup.find_all(text=True)\n",
    "    for t in text:\n",
    "        if t.parent.name not in blacklist:\n",
    "            output += '{} '.format(t)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thtml2ttext(thtml):\n",
    "    Output = []\n",
    "    for html in thtml:\n",
    "        text =  chap2text(html)\n",
    "        Output.append(text)\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def epub2text(epub_path):\n",
    "        chapters = epub2thtml(epub_path)\n",
    "        ttext = thtml2ttext(chapters)\n",
    "        return ttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=epub2text('./Murder_on_the_Orient_Express.epub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_one = out[5:13]\n",
    "part_two = out[14:29] \n",
    "part_three = out[30:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_path = \"./raw_text/\"\n",
    "# def save_chapter_by_part(part, chapters):\n",
    "#     for i in range(len(chapters)):\n",
    "#         file_name = \"Part{0}_Chapter{1}\".format(part, i+1)\n",
    "#         with open(fold_path+file_name, \"w\") as f:\n",
    "#             f.write(chapters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_chapter_by_part(1,part_one)\n",
    "# save_chapter_by_part(2,part_two)\n",
    "# save_chapter_by_part(3,part_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 大写转小写, 删除掉文本中的对话内容\n",
    "---\n",
    "这样做的原因是对话中往往含有很多法语，并且会有很多人名干扰词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = part_one[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_text = example_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17244"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def exclude_dialog(text):\n",
    "    print(\"all text length: \", len(text))\n",
    "    code_pattern = re.compile(r'“.*?”', flags=re.DOTALL)\n",
    "    res = code_pattern.sub(\" \", text)\n",
    "    print(\"without dialog length: \", len(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all text length:  17244\n",
      "without dialog length:  8024\n"
     ]
    }
   ],
   "source": [
    "res = exclude_dialog(example_text) # 删除对话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 处理缩写单词，去除标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "def clean_text(text):\n",
    "#     text = text.lower()\n",
    "    text = re.sub(r'\\b[A-Z][a-zA-Z]*\\b', \" \", text)    # 删除大写单词\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-—“”’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_list_noempty(mylist):\n",
    "    newlist = [item.strip() if hasattr(item, 'strip') else item for item in mylist]\n",
    "    return [item for item in newlist if item != '']\n",
    "\n",
    "def clean_punct(text, type = \"str\"): \n",
    "#     words = token.tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    punctuation_filtered = []\n",
    "    regex = re.compile('[%s]' % re.escape(punct))\n",
    "    for w in words:\n",
    "        punctuation_filtered.append(regex.sub('', w))\n",
    "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
    "    if type == \"str\":\n",
    "        return ' '.join(map(str, filtered_list))\n",
    "    else:\n",
    "        return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = clean_text(example_text)   # 处理缩写、处理残缺单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words_list = clean_punct(example_text, 'list')      # 清除标点， 保存成list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = clean_punct(example_text)                    # 清除标点， 保存成str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这一步进行NER，帮助筛选数据\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(example_text)\n",
    "NE = set(word for X in doc.ents for word in X.text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2578"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter_before = Counter(example_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 词型还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=WordNetLemmatizer()\n",
    "def lemitizeWords(text, type=\"str\"):\n",
    "    words=word_tokenize(text)\n",
    "    listLemma=[]\n",
    "    for w in words:\n",
    "        x=lemma.lemmatize(w, pos=\"v\")\n",
    "        listLemma.append(x)\n",
    "    if type == \"str\":\n",
    "        return ' '.join(map(str, listLemma))\n",
    "    else:\n",
    "        return listLemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemma_result_list = lemitizeWords(example_text, type = \"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = lemitizeWords(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_lemma = Counter(lemma_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 去除停顿词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def stopWordsRemove(text, type = \"str\"):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words=word_tokenize(text)\n",
    "    filtered = [w for w in words if not w in stop_words]\n",
    "    if type == \"str\":\n",
    "        return ' '.join(map(str, filtered))\n",
    "    else:\n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stop_result_list = stopWordsRemove(example_text, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = stopWordsRemove(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_stop_list = Counter(remove_stop_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter_stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dic = dict(counter_stop_list)      # 删除停顿词后的单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 基于词频过滤，得到最终的词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import zipf_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for w, c in word_dic.items():\n",
    "    word_freq[w] = zipf_frequency(w, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据对单词的认识程度，发现需要预先学习的单词的log词频应该在4.15一下 (词频为0的单词除外)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_level = 4.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 1,\n",
       " 'find': 2,\n",
       " 'difficult': 3,\n",
       " 'go': 7,\n",
       " 'sleep': 3,\n",
       " 'one': 13,\n",
       " 'thing': 3,\n",
       " 'miss': 2,\n",
       " 'motion': 1,\n",
       " 'train': 21,\n",
       " 'station': 2,\n",
       " 'outside': 3,\n",
       " 'curiously': 1,\n",
       " 'quiet': 2,\n",
       " 'contrast': 1,\n",
       " 'noise': 2,\n",
       " 'seem': 6,\n",
       " 'unusually': 1,\n",
       " 'loud': 1,\n",
       " 'could': 1,\n",
       " 'hear': 8,\n",
       " 'move': 2,\n",
       " 'next': 3,\n",
       " 'doora': 1,\n",
       " 'click': 2,\n",
       " 'pull': 1,\n",
       " 'washbasin': 1,\n",
       " 'sound': 3,\n",
       " 'tap': 3,\n",
       " 'run': 5,\n",
       " 'splash': 1,\n",
       " 'another': 1,\n",
       " 'basin': 1,\n",
       " 'shut': 2,\n",
       " 'pass': 3,\n",
       " 'corridor': 4,\n",
       " 'shuffle': 1,\n",
       " 'footsteps': 2,\n",
       " 'someone': 2,\n",
       " 'bedroom': 1,\n",
       " 'slippers': 1,\n",
       " 'lay': 3,\n",
       " 'awake': 2,\n",
       " 'star': 1,\n",
       " 'ceiling': 1,\n",
       " 'silent': 1,\n",
       " 'throat': 1,\n",
       " 'felt': 2,\n",
       " 'dry': 1,\n",
       " 'forget': 2,\n",
       " 'ask': 11,\n",
       " 'usual': 1,\n",
       " 'bottle': 1,\n",
       " 'mineral': 2,\n",
       " 'water': 5,\n",
       " 'look': 15,\n",
       " 'watch': 2,\n",
       " 'quarter': 2,\n",
       " 'past': 4,\n",
       " 'would': 8,\n",
       " 'ring': 1,\n",
       " 'conductor': 13,\n",
       " 'finger': 4,\n",
       " 'bell': 3,\n",
       " 'pause': 1,\n",
       " 'stillness': 1,\n",
       " 'ting': 1,\n",
       " 'man': 21,\n",
       " 'answer': 3,\n",
       " 'every': 1,\n",
       " '…ting…ting…': 1,\n",
       " 'get': 6,\n",
       " 'impatient': 2,\n",
       " '…': 1,\n",
       " 'keep': 1,\n",
       " 'solidly': 1,\n",
       " 'push': 1,\n",
       " 'rush': 1,\n",
       " 'echo': 1,\n",
       " 'aisle': 1,\n",
       " 'come': 9,\n",
       " 'knock': 1,\n",
       " 'door': 8,\n",
       " 'far': 5,\n",
       " 'voicesthe': 1,\n",
       " 'deferential': 1,\n",
       " 'apologetic': 1,\n",
       " 'woman': 7,\n",
       " 'sinsistent': 1,\n",
       " 'voluble': 1,\n",
       " 'smile': 2,\n",
       " 'altercationif': 1,\n",
       " 'onewent': 1,\n",
       " 'time': 10,\n",
       " 'proportion': 1,\n",
       " 'ninety': 1,\n",
       " 'per': 2,\n",
       " 'cent': 2,\n",
       " 'soothe': 1,\n",
       " 'ten': 2,\n",
       " 'matter': 3,\n",
       " 'adjust': 1,\n",
       " 'distinctly': 1,\n",
       " 'nuit': 1,\n",
       " 'close': 1,\n",
       " 'press': 1,\n",
       " 'arrive': 2,\n",
       " 'promptly': 1,\n",
       " 'hot': 1,\n",
       " 'worry': 2,\n",
       " 'l': 1,\n",
       " 'eau': 1,\n",
       " 'minerale': 1,\n",
       " 'il': 1,\n",
       " 'vous': 1,\n",
       " 'plait': 1,\n",
       " 'twinkle': 1,\n",
       " 'eye': 4,\n",
       " 'lead': 2,\n",
       " 'unburden': 1,\n",
       " 'wipe': 2,\n",
       " 'forehead': 1,\n",
       " 'insistsbut': 1,\n",
       " 'insist': 2,\n",
       " 'compartment': 6,\n",
       " 'space': 1,\n",
       " 'size': 2,\n",
       " 'sweep': 1,\n",
       " 'hand': 5,\n",
       " 'round': 1,\n",
       " 'conceal': 1,\n",
       " 'argue': 1,\n",
       " 'point': 1,\n",
       " 'impossible': 1,\n",
       " 'wake': 2,\n",
       " 'leave': 5,\n",
       " 'bolt': 1,\n",
       " 'behind': 1,\n",
       " 'listen': 2,\n",
       " 'reason': 1,\n",
       " 'though': 4,\n",
       " 'enough': 1,\n",
       " 'us': 4,\n",
       " 'already': 1,\n",
       " 'snow': 7,\n",
       " 'yes': 1,\n",
       " 'notice': 3,\n",
       " 'stop': 1,\n",
       " 'snowdrift': 2,\n",
       " 'know': 14,\n",
       " 'long': 2,\n",
       " 'shall': 2,\n",
       " 'remember': 1,\n",
       " 'seven': 1,\n",
       " 'days': 4,\n",
       " 'Là': 1,\n",
       " 'là': 1,\n",
       " 'say': 42,\n",
       " 'vexedly': 1,\n",
       " 'withdraw': 1,\n",
       " 'return': 2,\n",
       " 'soir': 1,\n",
       " 'drink': 1,\n",
       " 'glass': 1,\n",
       " 'compose': 1,\n",
       " 'drop': 1,\n",
       " 'something': 7,\n",
       " 'heavy': 2,\n",
       " 'fall': 1,\n",
       " 'thud': 1,\n",
       " 'spring': 1,\n",
       " 'open': 6,\n",
       " 'right': 2,\n",
       " 'way': 5,\n",
       " 'wrap': 1,\n",
       " 'scarlet': 1,\n",
       " 'kimono': 1,\n",
       " 'retreat': 1,\n",
       " 'end': 1,\n",
       " 'sit': 6,\n",
       " 'little': 5,\n",
       " 'seat': 3,\n",
       " 'enter': 3,\n",
       " 'figure': 1,\n",
       " 'large': 3,\n",
       " 'sheet': 1,\n",
       " 'paper': 1,\n",
       " 'deathly': 1,\n",
       " 'suffer': 1,\n",
       " 'nerve': 1,\n",
       " 'retire': 1,\n",
       " 'bed': 1,\n",
       " 'till': 1,\n",
       " 'morning': 6,\n",
       " 'still': 3,\n",
       " 'standstill': 2,\n",
       " 'raise': 1,\n",
       " 'blind': 2,\n",
       " 'bank': 1,\n",
       " 'surround': 1,\n",
       " 'glance': 2,\n",
       " 'saw': 1,\n",
       " 'nine': 1,\n",
       " 'clock': 2,\n",
       " 'neat': 1,\n",
       " 'spruce': 1,\n",
       " 'dandify': 1,\n",
       " 'ever': 1,\n",
       " 'make': 2,\n",
       " 'restaurant': 3,\n",
       " 'car': 7,\n",
       " 'chorus': 1,\n",
       " 'woe': 1,\n",
       " 'barriers': 1,\n",
       " 'might': 4,\n",
       " 'passengers': 3,\n",
       " 'quite': 3,\n",
       " 'break': 2,\n",
       " 'unite': 1,\n",
       " 'common': 2,\n",
       " 'misfortune': 1,\n",
       " 'loudest': 1,\n",
       " 'lamentations': 1,\n",
       " 'daughter': 2,\n",
       " 'easiest': 1,\n",
       " 'world': 1,\n",
       " 'may': 5,\n",
       " 'wail': 1,\n",
       " 'boat': 1,\n",
       " 'sail': 1,\n",
       " 'day': 1,\n",
       " 'tomorrow': 1,\n",
       " 'catch': 1,\n",
       " 'even': 2,\n",
       " 'wire': 1,\n",
       " 'cancel': 1,\n",
       " 'passage': 1,\n",
       " 'feel': 1,\n",
       " 'mad': 1,\n",
       " 'talk': 2,\n",
       " 'urgent': 1,\n",
       " 'business': 1,\n",
       " 'bad': 2,\n",
       " 'soothingly': 1,\n",
       " 'express': 3,\n",
       " 'hope': 1,\n",
       " 'sisterher': 1,\n",
       " 'children': 1,\n",
       " 'wait': 1,\n",
       " 'lady': 7,\n",
       " 'weep': 1,\n",
       " 'word': 1,\n",
       " 'think': 8,\n",
       " 'things': 2,\n",
       " 'happen': 3,\n",
       " 'demand': 2,\n",
       " 'anybody': 2,\n",
       " 'voice': 2,\n",
       " 'note': 3,\n",
       " 'sign': 1,\n",
       " 'almost': 1,\n",
       " 'feverish': 1,\n",
       " 'anxiety': 1,\n",
       " 'display': 1,\n",
       " 'check': 1,\n",
       " 'nobody': 1,\n",
       " 'try': 2,\n",
       " 'anything': 1,\n",
       " 'pack': 1,\n",
       " 'useless': 2,\n",
       " 'foreigners': 1,\n",
       " 'home': 1,\n",
       " 'least': 1,\n",
       " 'turn': 3,\n",
       " 'speak': 9,\n",
       " 'careful': 1,\n",
       " 'êtes': 1,\n",
       " 'un': 1,\n",
       " 'directeur': 1,\n",
       " 'de': 7,\n",
       " 'la': 1,\n",
       " 'ligne': 1,\n",
       " 'je': 1,\n",
       " 'crois': 1,\n",
       " 'pouvez': 1,\n",
       " 'nous': 1,\n",
       " 'dire': 1,\n",
       " 'correct': 2,\n",
       " 'confound': 1,\n",
       " 'friend': 7,\n",
       " 'sorry': 1,\n",
       " 'natural': 1,\n",
       " 'formerly': 1,\n",
       " 'present': 2,\n",
       " 'else': 1,\n",
       " 'absent': 1,\n",
       " 'couple': 1,\n",
       " 'valet': 2,\n",
       " 'maid': 1,\n",
       " 'foolish': 1,\n",
       " 'baby': 1,\n",
       " 'cry': 2,\n",
       " 'best': 1,\n",
       " 'whatever': 1,\n",
       " 'spirit': 1,\n",
       " 'however': 1,\n",
       " 'share': 1,\n",
       " 'well': 3,\n",
       " 'restlessly': 1,\n",
       " 'country': 2,\n",
       " 'anyway': 1,\n",
       " 'tearfully': 1,\n",
       " 'tell': 2,\n",
       " 'expect': 1,\n",
       " 'patient': 1,\n",
       " 'shrug': 1,\n",
       " 'shoulder': 1,\n",
       " 'slightly': 3,\n",
       " 'philosopher': 1,\n",
       " 'imply': 1,\n",
       " 'detach': 1,\n",
       " 'attitude': 2,\n",
       " 'selfish': 1,\n",
       " 'learn': 1,\n",
       " 'save': 1,\n",
       " 'emotion': 1,\n",
       " 'gaze': 1,\n",
       " 'window': 6,\n",
       " 'mass': 1,\n",
       " 'strong': 2,\n",
       " 'character': 3,\n",
       " 'gently': 1,\n",
       " 'strongest': 1,\n",
       " 'amongst': 2,\n",
       " 'indeed': 1,\n",
       " 'stronger': 1,\n",
       " 'suddenly': 1,\n",
       " 'realize': 1,\n",
       " 'stranger': 1,\n",
       " 'foreigner': 1,\n",
       " 'exchange': 1,\n",
       " 'half': 4,\n",
       " 'dozen': 1,\n",
       " 'sentence': 1,\n",
       " 'laugh': 3,\n",
       " 'polite': 2,\n",
       " 'estrange': 1,\n",
       " 'old': 3,\n",
       " 'instance': 1,\n",
       " 'probably': 1,\n",
       " 'ugly': 1,\n",
       " 'rather': 1,\n",
       " 'fascinate': 1,\n",
       " 'lift': 1,\n",
       " 'voiceand': 1,\n",
       " 'whole': 1,\n",
       " 'also': 2,\n",
       " 'director': 1,\n",
       " 'line': 1,\n",
       " 'masterful': 1,\n",
       " 'wear': 1,\n",
       " 'away': 1,\n",
       " 'people': 3,\n",
       " 'remain': 1,\n",
       " 'din': 3,\n",
       " 'communal': 1,\n",
       " 'life': 2,\n",
       " 'moment': 1,\n",
       " 'better': 2,\n",
       " 'good': 3,\n",
       " 'deal': 1,\n",
       " 'lifelong': 1,\n",
       " 'habit': 2,\n",
       " 'decease': 1,\n",
       " 'rise': 2,\n",
       " 'commence': 1,\n",
       " 'breakfast': 1,\n",
       " 'cereal': 1,\n",
       " 'final': 1,\n",
       " 'rest': 1,\n",
       " 'night': 2,\n",
       " 'bedsocks': 1,\n",
       " 'knit': 1,\n",
       " 'confuse': 1,\n",
       " 'account': 1,\n",
       " 'missionary': 1,\n",
       " 'aim': 1,\n",
       " 'conductors': 1,\n",
       " 'stand': 2,\n",
       " 'elbow': 1,\n",
       " 'compliment': 1,\n",
       " 'glad': 1,\n",
       " 'kind': 2,\n",
       " 'minutes': 2,\n",
       " 'utter': 1,\n",
       " 'excuse': 1,\n",
       " 'follow': 2,\n",
       " 'big': 2,\n",
       " 'fair': 1,\n",
       " 'guide': 1,\n",
       " 'carriage': 1,\n",
       " 'along': 2,\n",
       " 'aside': 1,\n",
       " 'let': 2,\n",
       " 'secondclass': 1,\n",
       " 'onechosen': 1,\n",
       " 'presumably': 1,\n",
       " 'larger': 1,\n",
       " 'certainly': 1,\n",
       " 'give': 3,\n",
       " 'impression': 1,\n",
       " 'crowd': 1,\n",
       " 'small': 2,\n",
       " 'opposite': 1,\n",
       " 'corner': 3,\n",
       " 'face': 7,\n",
       " 'dark': 2,\n",
       " 'prevent': 1,\n",
       " 'advance': 1,\n",
       " 'farther': 1,\n",
       " 'blue': 1,\n",
       " 'uniform': 1,\n",
       " 'chef': 6,\n",
       " 'need': 1,\n",
       " 'shift': 1,\n",
       " 'squeeze': 1,\n",
       " 'two': 3,\n",
       " 'men': 1,\n",
       " 'expression': 2,\n",
       " 'furiously': 1,\n",
       " 'clear': 1,\n",
       " 'occur': 5,\n",
       " 'snowthis': 1,\n",
       " 'stoppage': 1,\n",
       " 'pausedand': 1,\n",
       " 'sort': 1,\n",
       " 'strangle': 1,\n",
       " 'gasp': 1,\n",
       " 'passenger': 3,\n",
       " 'lie': 3,\n",
       " 'dead': 1,\n",
       " 'berthstabbed': 1,\n",
       " 'calm': 1,\n",
       " 'desperation': 1,\n",
       " 'calledcalled': 1,\n",
       " 'consult': 1,\n",
       " 'front': 1,\n",
       " 'gulp': 1,\n",
       " 'white': 1,\n",
       " 'chalk': 1,\n",
       " 'faint': 1,\n",
       " 'otherwise': 1,\n",
       " 'sink': 1,\n",
       " 'bury': 2,\n",
       " 'serious': 4,\n",
       " 'begin': 2,\n",
       " 'murderthat': 1,\n",
       " 'calamity': 1,\n",
       " 'first': 2,\n",
       " 'circumstances': 1,\n",
       " 'unusual': 1,\n",
       " 'bring': 1,\n",
       " 'hoursand': 1,\n",
       " 'hoursdays': 1,\n",
       " 'circumstance': 1,\n",
       " 'countries': 1,\n",
       " 'police': 2,\n",
       " 'comprehend': 3,\n",
       " 'position': 1,\n",
       " 'great': 3,\n",
       " 'difficulty': 1,\n",
       " 'worse': 1,\n",
       " 'introduce': 1,\n",
       " 'bow': 1,\n",
       " 'opinion': 2,\n",
       " 'death': 2,\n",
       " '1': 1,\n",
       " 'exactly': 2,\n",
       " 'doctor': 4,\n",
       " 'definitely': 1,\n",
       " 'midnight': 2,\n",
       " 'last': 4,\n",
       " 'see': 3,\n",
       " 'alive': 2,\n",
       " 'twenty': 1,\n",
       " 'toward': 1,\n",
       " 'continue': 1,\n",
       " 'wide': 1,\n",
       " 'suppose': 2,\n",
       " 'murderer': 3,\n",
       " 'escape': 1,\n",
       " 'depart': 1,\n",
       " 'distinct': 1,\n",
       " 'trace': 1,\n",
       " 'none': 1,\n",
       " 'crime': 2,\n",
       " 'discoveredwhen': 1,\n",
       " 'pale': 1,\n",
       " 'frighten': 1,\n",
       " 'gentleman': 3,\n",
       " 'order': 1,\n",
       " 'somewhat': 1,\n",
       " 'jerkily': 1,\n",
       " 'several': 2,\n",
       " 'hour': 3,\n",
       " 'ago': 2,\n",
       " 'attendant': 1,\n",
       " 'want': 1,\n",
       " 'take': 2,\n",
       " 'déjeuner': 1,\n",
       " 'eleven': 1,\n",
       " 'key': 1,\n",
       " 'chain': 3,\n",
       " 'fasten': 1,\n",
       " 'coldbut': 1,\n",
       " 'cold': 1,\n",
       " 'drift': 1,\n",
       " 'fit': 1,\n",
       " 'perhaps': 2,\n",
       " 'c': 1,\n",
       " 'était': 1,\n",
       " 'terrible': 2,\n",
       " 'lock': 2,\n",
       " 'inside': 1,\n",
       " 'thoughtfully': 2,\n",
       " 'suicideeh': 1,\n",
       " 'sardonic': 1,\n",
       " 'commit': 1,\n",
       " 'suicide': 1,\n",
       " 'stab': 2,\n",
       " 'tentwelvefifteen': 1,\n",
       " 'place': 2,\n",
       " 'ferocity': 1,\n",
       " 'upon': 1,\n",
       " 'like': 4,\n",
       " 'screw': 1,\n",
       " 'must': 2,\n",
       " 'desire': 1,\n",
       " 'technicallythat': 1,\n",
       " 'confusingbut': 1,\n",
       " 'assure': 2,\n",
       " 'blow': 2,\n",
       " 'deliver': 2,\n",
       " 'force': 1,\n",
       " 'drive': 1,\n",
       " 'hard': 1,\n",
       " 'belt': 1,\n",
       " 'bone': 1,\n",
       " 'muscle': 1,\n",
       " 'clearly': 1,\n",
       " 'scientific': 1,\n",
       " 'unscientific': 1,\n",
       " 'haphazard': 1,\n",
       " 'random': 1,\n",
       " 'hardly': 1,\n",
       " 'damage': 1,\n",
       " 'somebody': 1,\n",
       " 'frenzy': 1,\n",
       " 'strike': 1,\n",
       " 'blindly': 1,\n",
       " 'est': 2,\n",
       " 'une': 1,\n",
       " 'femme': 1,\n",
       " 'enrage': 1,\n",
       " 'strength': 1,\n",
       " 'nod': 3,\n",
       " 'sagely': 1,\n",
       " 'everyone': 1,\n",
       " 'suspect': 1,\n",
       " 'personal': 1,\n",
       " 'experience': 1,\n",
       " 'contribute': 1,\n",
       " 'store': 1,\n",
       " 'knowledge': 1,\n",
       " 'yesterday': 1,\n",
       " 'able': 1,\n",
       " 'understand': 1,\n",
       " 'danger': 1,\n",
       " '‘': 4,\n",
       " 'gunman': 1,\n",
       " 'pain': 1,\n",
       " 'theory': 1,\n",
       " 'naught': 1,\n",
       " 'amateurishly': 1,\n",
       " 'tone': 1,\n",
       " 'professional': 1,\n",
       " 'disapproval': 1,\n",
       " 'pursue': 1,\n",
       " 'idea': 1,\n",
       " 'commonlooking': 1,\n",
       " 'clothe': 1,\n",
       " 'chew': 1,\n",
       " 'gum': 1,\n",
       " 'believe': 1,\n",
       " 'circle': 1,\n",
       " 'mean': 1,\n",
       " 'appeal': 1,\n",
       " '16': 1,\n",
       " 'presently': 1,\n",
       " 'question': 1,\n",
       " 'back': 3,\n",
       " 'power': 1,\n",
       " 'command': 1,\n",
       " 'investigation': 1,\n",
       " 'refuse': 1,\n",
       " 'des': 1,\n",
       " 'simple': 1,\n",
       " 'solution': 1,\n",
       " 'delay': 1,\n",
       " 'annoyances': 1,\n",
       " 'million': 1,\n",
       " 'inconvenience': 2,\n",
       " 'annoyance': 1,\n",
       " 'innocent': 1,\n",
       " 'persons': 1,\n",
       " 'solve': 3,\n",
       " 'mystery': 1,\n",
       " 'murder': 1,\n",
       " 'criminal': 1,\n",
       " 'mon': 1,\n",
       " 'cher': 1,\n",
       " 'become': 1,\n",
       " 'positively': 1,\n",
       " 'caress': 1,\n",
       " 'reputation': 1,\n",
       " 'methods': 1,\n",
       " 'ideal': 1,\n",
       " 'case': 3,\n",
       " 'antecedents': 1,\n",
       " 'discover': 1,\n",
       " 'bona': 1,\n",
       " 'fides': 1,\n",
       " 'endless': 1,\n",
       " 'often': 2,\n",
       " 'chair': 1,\n",
       " 'view': 1,\n",
       " 'body': 1,\n",
       " 'examine': 1,\n",
       " 'clue': 1,\n",
       " 'thenwell': 1,\n",
       " 'faith': 2,\n",
       " 'idle': 1,\n",
       " 'boast': 1,\n",
       " 'thinkuse': 1,\n",
       " 'grey': 1,\n",
       " 'cells': 1,\n",
       " 'mindand': 1,\n",
       " 'lean': 1,\n",
       " 'forward': 1,\n",
       " 'affectionately': 1,\n",
       " 'touch': 1,\n",
       " 'emotionally': 1,\n",
       " 'nightbut': 1,\n",
       " 'truth': 1,\n",
       " 'problem': 2,\n",
       " 'intrigue': 1,\n",
       " 'reflect': 1,\n",
       " 'many': 1,\n",
       " 'hours': 1,\n",
       " 'boredom': 1,\n",
       " 'ahead': 1,\n",
       " 'whilst': 1,\n",
       " 'stick': 1,\n",
       " 'nowa': 1,\n",
       " 'ready': 1,\n",
       " 'accept': 1,\n",
       " 'eagerly': 1,\n",
       " 'entendu': 1,\n",
       " 'service': 1,\n",
       " 'plan': 1,\n",
       " 'coach': 5,\n",
       " 'occupy': 1,\n",
       " 'compartments': 1,\n",
       " 'passports': 1,\n",
       " 'ticket': 1,\n",
       " 'travellers': 1,\n",
       " 'lame': 1,\n",
       " 'leg': 1,\n",
       " 'ordinary': 1,\n",
       " 'carriages': 1,\n",
       " 'concern': 1,\n",
       " 'since': 2,\n",
       " 'dinner': 1,\n",
       " 'serve': 1,\n",
       " 'slowly': 1,\n",
       " 'hint': 1,\n",
       " 'solemnly': 1,\n",
       " 'uson': 1,\n",
       " 'now…': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuable_word = []\n",
    "for w, f in word_freq.items():\n",
    "    if 0 < f < my_level and w not in NE and word_dic[w]>1:\n",
    "        valuable_word.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valuable_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from urllib.parse import urlparse, quote, urlencode, unquote\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    " \n",
    "def fetch(query_str):\n",
    "    query = {'q': \"\".join(query_str)}   # list --> str: \"\".join(list)\n",
    "    url = 'https://fanyi.youdao.com/openapi.do?keyfrom=11pegasus11&key=273646050&type=data&doctype=json&version=1.1&' + urlencode(query)\n",
    "    response = urlopen(url, timeout=3)\n",
    "    html = response.read().decode('utf-8')\n",
    "    return html\n",
    "\n",
    "def prase(html):\n",
    "    prased = json.loads(html)\n",
    "    if 'basic' in prased and 'explains'in prased['basic']:\n",
    "        explain_list = prased['basic']['explains']\n",
    "        for i in range(len(explain_list)):\n",
    "            if \"人名\" in explain_list[i] or \"地名\" in explain_list[i]:    # 将人名地名的翻译删掉\n",
    "                explain_list[i] = \"\"\n",
    "        return \" \".join(prased['basic']['explains'])\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 翻译单词\n",
    "import time\n",
    "vocabulary = []\n",
    "for w in valuable_word:\n",
    "    vocabulary.append([w, prase(fetch(w))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corridor\t\tn. 走廊\n",
      "footsteps\t\tn. 脚步（footstep的复数形式）；步距\n",
      "mineral\t\tn. 矿物；（英）矿泉水；无机物；苏打水（常用复数表示） adj. 矿物的；矿质的\n",
      "conductor\t\tn. 导体；售票员；领导者；管理人\n",
      "impatient\t\tadj. 焦躁的；不耐心的\n",
      "wipe\t\tn. 擦拭；用力打 vi. 擦；打 vt. 擦；消除；涂上\n",
      "insist\t\tvi. 坚持，强调 vt. 坚持，强调\n",
      "compartment\t\tn. [建] 隔间；区划；卧车上的小客房 vt. 分隔；划分\n",
      "snowdrift\t\tn. 随风飘飞的雪；被风刮在一起的雪堆\n",
      "standstill\t\tn. 停顿；停止\n",
      "glance\t\tv. 瞥闪，瞥见，扫视，匆匆一看；浏览；斜击某物后斜弹开来；反光；轻擦（球）；（板球）斜击  n. 一瞥；一滑；闪光；（板球）斜击；辉金属\n",
      "valet\t\tn. 贴身男仆；用车的人；伺候客人停车 vt. 为...管理衣物；替...洗熨衣服 vi. 清洗汽车；服侍 \n",
      "polite\t\tadj. 有礼貌的，客气的；文雅的；上流的；优雅的\n",
      "din\t\t n. 喧嚣声，嘈杂声；宗教法律，犹太法律；（尤指伊斯兰教）宗教信仰 v. 再三叮嘱，反复教导；发出喧闹声\n",
      "bury\t\tvt. 埋葬；隐藏 \n",
      "comprehend\t\tvt. 理解；包含；由…组成\n",
      "murderer\t\tn. 凶手；谋杀犯\n",
      "thoughtfully\t\tadv. 沉思地；体贴地，亲切地\n",
      "stab\t\tn. 刺；戳；尝试；突发的一阵 vt. 刺；刺伤；戳；刺穿；直入 vi. 刺；刺伤；戳；刺痛 \n",
      "assure\t\tvt. 保证；担保；使确信；弄清楚\n",
      "nod\t\tv. 点头；点头表示；打盹；摆动；（英足）顶球 n. 点头；点头表示；打盹；摆动；同意，让步\n",
      "inconvenience\t\tn. 不便；麻烦 vt. 麻烦；打扰\n"
     ]
    }
   ],
   "source": [
    "for item in vocabulary:\n",
    "    print(\"{}\\t\\t{}\".format(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadHelper:\n",
    "    def __init__(book_path, language_level):\n",
    "        this.book_path = book_path\n",
    "        this.language_level = language_level\n",
    "        \n",
    "    def\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
